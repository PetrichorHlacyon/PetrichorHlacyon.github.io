四种Mathematical programming，
- Linear programming(LP)
- Integer programming(IP)
- Quadratic programming(QP)
- Nonlinear programming(NP)
- Dynamic programming(DP)：比如节点间相互通信。


# Unconstrainted Optimization
# Conjugate Method and Quasi-Newton Method
优化中Algrithm是指解决某类优化问题的具体算法，而Method则是指某个思想。


共轭方法严重依赖初始解，特别是条件数特别大的时候，对初始解的选择较为严苛
拟牛顿法是在牛顿法的基础上提出的，通过拟牛顿条件将Hessian矩阵近似，可以获得较快的收敛速度的较为松弛的初始条件。
拟牛顿法不仅可以产生共轭的方向，而且可以随着迭代次数增加，逐渐逼近Hessian矩阵

基于梯度的算法，特别是目标函数可微的问题，无非用到一阶偏导，二阶偏导的信息，共轭方法用到一阶偏导的信息，拟牛顿法和牛顿法用到二阶偏导的信息。使用二阶偏导的算法由于利用了二阶信息，所以收敛率较快，但是存在每次迭代计算量大的问题，拟牛顿法在牛顿法的基础上进行了近似，因此计算量有所减少，但是仍然比共轭方法计算量大，因此当问题维度n较大是，优先考虑使用共轭方法，减少计算量。

# Nonderivative Methods
同样也是利用近似，利用差分近似一阶偏导二阶偏导。
## Coordinate Descent
该方法适用于可分离的目标函数，也就是目标函数$f(x)$可以分解成$f(x)=\sum_{i=0}^{n-1}f_i(x)$的情况，每次优化只沿着一个坐标轴优化，可以写成
$$
x_i^{k+1} = \mathop{\arg\min}_{x_i} f(x_1^k,x_2^k,...,x_{i-1}^k,x_i,x_{i+1}^k,...,x_n^k)
$$
由于这种可分离性质，某个大优化问题可以被分解成块，交给从机进行并行优化。
这种方法不仅适用于目标函数可微的情况，同时也是用于目标函数不可微但是可分离的情况，典型的如$l1-norm \,\,\,\sum_{i=1}^n |x_i|$.。

## Direct Search Methods
Coordinate Descent沿着固定坐标轴优化，坐标轴不会变化，那么为了加速收敛，提出了Direct Search Methods，它能产生变化方向的坐标轴，这会提高收敛率。
这类方法的收敛性在理论上不那么让人安心，但是他并不需要计算梯度。
***Nelder-Mead simplex Method***
不要和针对LP问题的单纯形法混淆。这种方法对于小维度问题有很好的效果，但是收敛性无法得到保证。

# Incremental Methods
研究如下目标函数问题，
$$
\min_{x} f(x)=\sum_{i=1}^n f_i(x)
$$
本章讲解的方法都是针对$f_i(x)$可微的问题。常见的机器学习问题有分类问题，比如,$$
\sum_{i=1}^n h(b_i(c_i^Tx+y))
$$
这里$b_i$代表标签，$c_i$代表数据特征，$h(\cdot)$是惩罚函数$penalty$ $function$，当$b_i(c_i^Tx+y)<0$惩罚函数会把它变成很大的值，当$b_i(c_i^Tx+y)>0$惩罚函数会把它变成很小的值，所以惩罚函数是一个单调不增的函数。

## Incremental Gradient Methods
考虑$f_i(x)$是连续可微函数，incremental methods有如下广义形式，
$$
x_{k+1} = x^k - \alpha^k \nabla f_{i_k}(x^k)
$$
其中$\alpha^k>0$，$i_k\in\{1,2,3,...,n\}$，$i_k$可以通过一些规则选取，可以分为deterministic和randomized。可以预见这种方法和随机算法有些类似，只能收敛到最优解的邻域内，因此***incremental Methods在远离最优解的时候有显著优势，也就是说在较少的迭代次数能够收敛到和原来相同的解；但是靠近最优解的时候性能较差，这时必须要衰减的步长才能收敛。***

对于单个函数如二次函数的那些，在region of confusion 外使用incremental methods以获得较快的收敛速度和差不读的收敛结果，当在region of confusion内才进行如steepest descent那些的算法以便收敛到确定的结果。

***step size***的选取极其重要，一般衰减的步长才能收敛到稳定点(stationary point),有时非常小的步长也可以收敛。当步长$\alpha$是小的，incremental和nonincremental有相同的异步收敛率，区别是incremental方法收敛到$x(\alpha)$，nonincremental收敛到$x^*$。

***Stochastic Gradient Methods*** 解决的问题尽管和incremental类似，但是他们俩并不等价，也不能随意转化，

- 随机梯度法采样空间可能是无限维的，而incremental方法的函数和是有限个函数加权和的。如果进行了转化可能会没有利用到incremental方法的问题特性。


# Optimization over a Convex Set
和无约束优化问题类似，都是沿着suitable的方向进行迭代下降，然而不同的是这些迭代的方向必须保持feasible。

## Feasible Direction-Conditional Gradient Method
和梯度下降法一样，都要选择搜索方向，但是这种方法选择的方向是feasible direction，也就是针对约束问题而言，当前解向feasible direction搜索仍然满足约束。解的迭代表达式为，
$$
x^{k+1} = x^k+\alpha^k (\bar{x}^k-x^k)
$$
和梯度下降法的区别就是搜索方向变了，这里$\bar{x}^k$满足$\nabla f(x^k)^T(\bar{x}^k-x^k)<0$。
***Stepsize Selection***：这里的步长选取可以用梯度下降法的步长选取方法

- Limited Minimization Rule。$\alpha^k=\mathop{\arg\min}_{\alpha}f(x^k+\alpha d^k)$。
- Armijo Rule。
- Constant Stepsize。

***Search Direction***：搜索方向选取不同就会产生不同的算法。
- Conditional Gradient Method（Frank-Wolfe Method）。$\bar{x}^k=\mathop{\arg\min}_{x\in X} \nabla f(x^k)^T(x-x^k)$。这种方法不仅要解origin problem还要解这个subproblem，所以subploblem必须容易解，也就是考虑$X$ is specified by linear constraints。

## Gradient Projection Methods
Conditional Gradient Method通过解线性cost的子问题得到search direction。Gradient Projection通过解Quadratic cost的子问题得到search direction。子问题变复杂了，收敛结果更好。待确定的搜索方向中未知量$\bar{x}^k$可以由下面式子确定，
$$\bar{x}^k=[x^k-s^k\nabla f(x^k)]^+$$
上式中$s^k$表示一个标量常数，$+$代表括号内的式子在集。因此该式变成了解一个二次的子问题$\bar{x}^k=\mathop{\arg\min}_x ||x-(x^k-s^k\nabla f(x^k))||^2$。

***Stepsize Selection***：类似前面的梯度法步长选取。

## Two-Metric Projection Methods
前面介绍的方法适用于projection容易求得的情况，为了解决前面方法求解慢，实现复杂的问题，提出了本节的方法，two-metric projection methods在scaled gradient 基础上增加了一个projection,
$$x^{k+1}=[x^k-\alpha^k D^k\nabla f(x^k)]^+$$
名字来自于：嵌入了两个矩阵，一个$D^k$用于scales梯度，一个用于投影计算。同时为了保证算法是下降的，对$D^k$的选择也有要求。

## Manifold Suboptimization Methods
该算法也属于带线性约束的***feasible direction methods***，这种方法也可以看成一种梯度投影法，不过投影不是投到整个可行解集合上，而是投影到linear manifold上，这极大减少了投影方法的计算量。
【TODO】这种方法和单纯形法很像，可以优先学习

## Proximal Algorithm❌❎
✅$\quad f(x)$没必要有梯度，使用proximal迭代，而不是approximate迭代，也就是不适用泰勒展开，因此算法可适用于***不可微问题***。
✅$\quad$添加一个proximal term相当于regularization，使得待优化的问题在compact level sets变成strictly convex。因此算法只适用于能受益于正则化的问题，

# Summary
Gradient Methods和Incremental Methods的解的形式有区别，区别在于，一个是固定方向，一个是每次按照一定规则选取方向的